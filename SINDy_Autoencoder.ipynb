{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OdrXpE_49LXV",
        "PyMjbuW26Rx-",
        "Omk20hNe61F8",
        "L6ky0yqP68fY",
        "38CXoiLg--Nc",
        "_d327kpBAaaB"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "import dill\n",
        "from tqdm import tqdm\n",
        "from scipy.integrate import odeint\n",
        "from scipy.special import legendre\n",
        "from itertools import combinations_with_replacement"
      ],
      "metadata": {
        "id": "b4E8EsK-7paC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "qDzxl4qs9ZrW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SINDy Utils"
      ],
      "metadata": {
        "id": "OdrXpE_49LXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sindy_library_pt(z, latent_dim, poly_order, include_sine=False):\n",
        "    \"\"\"\n",
        "    Description: Builds the SINDy library for a first-order dynamical system.\n",
        "\n",
        "    Args:\n",
        "        z (torch.Tensor): Input tensor of shape (batch_size, latent_dim), representing latent states.\n",
        "        latent_dim (int): Number of latent variables (dimensions).\n",
        "        poly_order (int): Maximum degree of polynomial terms to include in the library.\n",
        "        include_sine (bool): Whether to include sine terms in the library.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A matrix (batch_size, library_size) where each column is a function of z.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the library with a column of ones. The number of rows is equal to batch size.\n",
        "    library = [torch.ones(z.size(0)).to(device)]\n",
        "\n",
        "    # Prepare to loop over all variable combinations\n",
        "    sample_list = range(latent_dim)\n",
        "\n",
        "    for n in range(1, poly_order + 1):\n",
        "        # Get all combinations (with replacement) of latent_dim variables of total degree n\n",
        "        list_combinations = list(combinations_with_replacement(sample_list, n))\n",
        "\n",
        "        for combination in list_combinations:\n",
        "            # For each combination, compute the product of the corresponding columns in z\n",
        "            # e.g., z[:, [0, 0]] -> z_0^2, z[:, [1, 2]] -> z_1 * z_2\n",
        "            term = torch.prod(z[:, combination], dim=1)\n",
        "            library.append(term.to(device))  # Add to the library (on GPU)\n",
        "\n",
        "    # Optionally add sine terms of each latent variable\n",
        "    if include_sine:\n",
        "        for i in range(latent_dim):\n",
        "            library.append(\n",
        "                torch.sin(z[:, i])\n",
        "            )  # Automatically on correct device since z is\n",
        "\n",
        "    # Stack all features column-wise into a single tensor of shape (batch_size, num_features)\n",
        "    return torch.stack(library, dim=1).to(device)\n",
        "\n",
        "def sindy_library_pt_order2(z, dz, latent_dim, poly_order, include_sine=False):\n",
        "    \"\"\"\n",
        "    Build the SINDy library for a second-order system.\n",
        "    \"\"\"\n",
        "    library = [torch.ones(z.size(0)).to(device)]  # initialize library\n",
        "\n",
        "    # concatenate z and dz\n",
        "    z_combined = torch.cat([z, dz], dim=1)\n",
        "\n",
        "    sample_list = range(2 * latent_dim)\n",
        "    list_combinations = list()\n",
        "\n",
        "    for n in range(1, poly_order + 1):\n",
        "        list_combinations = list(combinations_with_replacement(sample_list, n))\n",
        "        for combination in list_combinations:\n",
        "            library.append(\n",
        "                torch.prod(z_combined[:, combination], dim=1).to(device)\n",
        "            )\n",
        "\n",
        "    # add sine terms if included\n",
        "    if include_sine:\n",
        "        for i in range(2 * latent_dim):\n",
        "            library.append(torch.sin(z_combined[:, i]))\n",
        "\n",
        "    return torch.stack(library, dim=1).to(device)\n",
        "\n",
        "def library_size(latent_dim, poly_order):\n",
        "    f = lambda latent_dim, poly_order: math.comb(\n",
        "        latent_dim + poly_order - 1, poly_order\n",
        "    )\n",
        "    total = 0\n",
        "    for i in range(poly_order + 1):\n",
        "        total += f(latent_dim, i)\n",
        "    return total"
      ],
      "metadata": {
        "id": "bsWmIbNY9Kxe"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate data"
      ],
      "metadata": {
        "id": "PyMjbuW26Rx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lorenz Code"
      ],
      "metadata": {
        "id": "Omk20hNe61F8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RbZPQelB56lX"
      },
      "outputs": [],
      "source": [
        "def get_lorenz_data(n_ics, noise_strength=0):\n",
        "    \"\"\"\n",
        "    Generate a set of Lorenz training data for multiple random initial conditions.\n",
        "\n",
        "    Arguments:\n",
        "        n_ics - Integer specifying the number of initial conditions to use.\n",
        "        noise_strength - Amount of noise to add to the data.\n",
        "\n",
        "    Return:\n",
        "        data - Dictionary containing elements of the dataset. See generate_lorenz_data()\n",
        "        doc string for list of contents.\n",
        "    \"\"\"\n",
        "    t = np.arange(0, 5, 0.02)\n",
        "    n_steps = t.size\n",
        "    input_dim = 128\n",
        "\n",
        "    ic_means = np.array([0, 0, 25])\n",
        "    ic_widths = 2 * np.array([36, 48, 41])\n",
        "\n",
        "    # training data\n",
        "    ics = ic_widths * (np.random.rand(n_ics, 3) - 0.5) + ic_means\n",
        "    data = generate_lorenz_data(\n",
        "        ics,\n",
        "        t,\n",
        "        input_dim,\n",
        "        linear=False,\n",
        "        normalization=np.array([1 / 40, 1 / 40, 1 / 40]),\n",
        "    )\n",
        "    data[\"x\"] = data[\"x\"].reshape((-1, input_dim)) + noise_strength * np.random.randn(\n",
        "        n_steps * n_ics, input_dim\n",
        "    )\n",
        "    data[\"dx\"] = data[\"dx\"].reshape((-1, input_dim)) + noise_strength * np.random.randn(\n",
        "        n_steps * n_ics, input_dim\n",
        "    )\n",
        "    data[\"ddx\"] = data[\"ddx\"].reshape(\n",
        "        (-1, input_dim)\n",
        "    ) + noise_strength * np.random.randn(n_steps * n_ics, input_dim)\n",
        "\n",
        "    return data\n",
        "\n",
        "def simulate_lorenz(z0, t, sigma=10.0, beta=8 / 3, rho=28.0):\n",
        "    \"\"\"\n",
        "    Simulate the Lorenz dynamics.\n",
        "\n",
        "    Arguments:\n",
        "        z0 - Initial condition in the form of a 3-value list or array.\n",
        "        t - Array of time points at which to simulate.\n",
        "        sigma, beta, rho - Lorenz parameters\n",
        "\n",
        "    Returns:\n",
        "        z, dz, ddz - Arrays of the trajectory values and their 1st and 2nd derivatives.\n",
        "    \"\"\"\n",
        "    f = lambda z, t: [\n",
        "        sigma * (z[1] - z[0]),\n",
        "        z[0] * (rho - z[2]) - z[1],\n",
        "        z[0] * z[1] - beta * z[2],\n",
        "    ]\n",
        "    df = lambda z, dz, t: [\n",
        "        sigma * (dz[1] - dz[0]),\n",
        "        dz[0] * (rho - z[2]) + z[0] * (-dz[2]) - dz[1],\n",
        "        dz[0] * z[1] + z[0] * dz[1] - beta * dz[2],\n",
        "    ]\n",
        "\n",
        "    z = odeint(f, z0, t)\n",
        "\n",
        "    dt = t[1] - t[0]\n",
        "    dz = np.zeros(z.shape)\n",
        "    ddz = np.zeros(z.shape)\n",
        "    for i in range(t.size):\n",
        "        dz[i] = f(z[i], dt * i)\n",
        "        ddz[i] = df(z[i], dz[i], dt * i)\n",
        "    return z, dz, ddz\n",
        "\n",
        "def lorenz_coefficients(normalization, poly_order=3, sigma=10.0, beta=8 / 3, rho=28.0):\n",
        "    \"\"\"\n",
        "    Generate the SINDy coefficient matrix for the Lorenz system.\n",
        "\n",
        "    Arguments:\n",
        "        normalization - 3-element list of array specifying scaling of each Lorenz variable\n",
        "        poly_order - Polynomial order of the SINDy model.\n",
        "        sigma, beta, rho - Parameters of the Lorenz system\n",
        "    \"\"\"\n",
        "    Xi = np.zeros((library_size(3, poly_order), 3))\n",
        "    Xi[1, 0] = -sigma\n",
        "    Xi[2, 0] = sigma * normalization[0] / normalization[1]\n",
        "    Xi[1, 1] = rho * normalization[1] / normalization[0]\n",
        "    Xi[2, 1] = -1\n",
        "    Xi[6, 1] = -normalization[1] / (normalization[0] * normalization[2])\n",
        "    Xi[3, 2] = -beta\n",
        "    Xi[5, 2] = normalization[2] / (normalization[0] * normalization[1])\n",
        "    return Xi\n",
        "\n",
        "def generate_lorenz_data(\n",
        "    ics, t, n_points, linear=True, normalization=None, sigma=10, beta=8 / 3, rho=28\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate high-dimensional Lorenz data set.\n",
        "\n",
        "    Arguments:\n",
        "        ics - Nx3 array of N initial conditions\n",
        "        t - array of time points over which to simulate\n",
        "        n_points - size of the high-dimensional dataset created\n",
        "        linear - Boolean value. If True, high-dimensional dataset is a linear combination\n",
        "        of the Lorenz dynamics. If False, the dataset also includes cubic modes.\n",
        "        normalization - Optional 3-value array for rescaling the 3 Lorenz variables.\n",
        "        sigma, beta, rho - Parameters of the Lorenz dynamics.\n",
        "\n",
        "    Returns:\n",
        "        data - Dictionary containing elements of the dataset. This includes the time points (t),\n",
        "        spatial mapping (y_spatial), high-dimensional modes used to generate the full dataset\n",
        "        (modes), low-dimensional Lorenz dynamics (z, along with 1st and 2nd derivatives dz and\n",
        "        ddz), high-dimensional dataset (x, along with 1st and 2nd derivatives dx and ddx), and\n",
        "        the true Lorenz coefficient matrix for SINDy.\n",
        "    \"\"\"\n",
        "\n",
        "    n_ics = ics.shape[0]\n",
        "    n_steps = t.size\n",
        "    dt = t[1] - t[0]\n",
        "\n",
        "    d = 3\n",
        "    z = np.zeros((n_ics, n_steps, d))\n",
        "    dz = np.zeros(z.shape)\n",
        "    ddz = np.zeros(z.shape)\n",
        "    for i in range(n_ics):\n",
        "        z[i], dz[i], ddz[i] = simulate_lorenz(\n",
        "            ics[i], t, sigma=sigma, beta=beta, rho=rho\n",
        "        )\n",
        "\n",
        "    if normalization is not None:\n",
        "        z *= normalization\n",
        "        dz *= normalization\n",
        "        ddz *= normalization\n",
        "\n",
        "    n = n_points\n",
        "    L = 1\n",
        "    y_spatial = np.linspace(-L, L, n)\n",
        "\n",
        "    modes = np.zeros((2 * d, n))\n",
        "    for i in range(2 * d):\n",
        "        modes[i] = legendre(i)(y_spatial)\n",
        "        # modes[i] = chebyt(i)(y_spatial)\n",
        "        # modes[i] = np.cos((i+1)*np.pi*y_spatial/2)\n",
        "    x1 = np.zeros((n_ics, n_steps, n))\n",
        "    x2 = np.zeros((n_ics, n_steps, n))\n",
        "    x3 = np.zeros((n_ics, n_steps, n))\n",
        "    x4 = np.zeros((n_ics, n_steps, n))\n",
        "    x5 = np.zeros((n_ics, n_steps, n))\n",
        "    x6 = np.zeros((n_ics, n_steps, n))\n",
        "\n",
        "    x = np.zeros((n_ics, n_steps, n))\n",
        "    dx = np.zeros(x.shape)\n",
        "    ddx = np.zeros(x.shape)\n",
        "    for i in range(n_ics):\n",
        "        for j in range(n_steps):\n",
        "            x1[i, j] = modes[0] * z[i, j, 0]\n",
        "            x2[i, j] = modes[1] * z[i, j, 1]\n",
        "            x3[i, j] = modes[2] * z[i, j, 2]\n",
        "            x4[i, j] = modes[3] * z[i, j, 0] ** 3\n",
        "            x5[i, j] = modes[4] * z[i, j, 1] ** 3\n",
        "            x6[i, j] = modes[5] * z[i, j, 2] ** 3\n",
        "\n",
        "            x[i, j] = x1[i, j] + x2[i, j] + x3[i, j]\n",
        "            if not linear:\n",
        "                x[i, j] += x4[i, j] + x5[i, j] + x6[i, j]\n",
        "\n",
        "            dx[i, j] = (\n",
        "                modes[0] * dz[i, j, 0] + modes[1] * dz[i, j, 1] + modes[2] * dz[i, j, 2]\n",
        "            )\n",
        "            if not linear:\n",
        "                dx[i, j] += (\n",
        "                    modes[3] * 3 * (z[i, j, 0] ** 2) * dz[i, j, 0]\n",
        "                    + modes[4] * 3 * (z[i, j, 1] ** 2) * dz[i, j, 1]\n",
        "                    + modes[5] * 3 * (z[i, j, 2] ** 2) * dz[i, j, 2]\n",
        "                )\n",
        "\n",
        "            ddx[i, j] = (\n",
        "                modes[0] * ddz[i, j, 0]\n",
        "                + modes[1] * ddz[i, j, 1]\n",
        "                + modes[2] * ddz[i, j, 2]\n",
        "            )\n",
        "            if not linear:\n",
        "                ddx[i, j] += (\n",
        "                    modes[3]\n",
        "                    * (\n",
        "                        6 * z[i, j, 0] * dz[i, j, 0] ** 2\n",
        "                        + 3 * (z[i, j, 0] ** 2) * ddz[i, j, 0]\n",
        "                    )\n",
        "                    + modes[4]\n",
        "                    * (\n",
        "                        6 * z[i, j, 1] * dz[i, j, 1] ** 2\n",
        "                        + 3 * (z[i, j, 1] ** 2) * ddz[i, j, 1]\n",
        "                    )\n",
        "                    + modes[5]\n",
        "                    * (\n",
        "                        6 * z[i, j, 2] * dz[i, j, 2] ** 2\n",
        "                        + 3 * (z[i, j, 2] ** 2) * ddz[i, j, 2]\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    if normalization is None:\n",
        "        sindy_coefficients = lorenz_coefficients(\n",
        "            [1, 1, 1], sigma=sigma, beta=beta, rho=rho\n",
        "        )\n",
        "    else:\n",
        "        sindy_coefficients = lorenz_coefficients(\n",
        "            normalization, sigma=sigma, beta=beta, rho=rho\n",
        "        )\n",
        "\n",
        "    data = {}\n",
        "    data[\"t\"] = t\n",
        "    data[\"y_spatial\"] = y_spatial\n",
        "    data[\"modes\"] = modes\n",
        "    data[\"x\"] = x\n",
        "    data[\"dx\"] = dx\n",
        "    data[\"ddx\"] = ddx\n",
        "    data[\"z\"] = z\n",
        "    data[\"dz\"] = dz\n",
        "    data[\"ddz\"] = ddz\n",
        "    data[\"sindy_coefficients\"] = sindy_coefficients.astype(np.float32)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lorenz Data"
      ],
      "metadata": {
        "id": "L6ky0yqP68fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_strength = 1e-6\n",
        "training_data = get_lorenz_data(10, noise_strength=noise_strength)\n",
        "validation_data = get_lorenz_data(5, noise_strength=noise_strength)\n",
        "test_data = get_lorenz_data(5, noise_strength=noise_strength)"
      ],
      "metadata": {
        "id": "YFOCEVZc7Ah_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "JIO1QaSO7PXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Linear Layer"
      ],
      "metadata": {
        "id": "38CXoiLg--Nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearLayer(torch.nn.Linear):\n",
        "    \"\"\"Custom implementation of layer which includes the activation function\n",
        "    and its derivative\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        activation_function: torch.nn.Module,\n",
        "        last: bool = False,\n",
        "        order: int = 1,\n",
        "        bias: bool = True,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "\n",
        "        # Constructor for a regular linear layer\n",
        "        super().__init__(in_features, out_features, bias, device, dtype=torch.float64)\n",
        "\n",
        "        # Our modifications to the linear layer\n",
        "\n",
        "        # Store the activation function\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "        # Store the activation function's first derivative\n",
        "        self.activation_derivative = self.__get_activation_derivative()\n",
        "\n",
        "        # Store the activation function'sclass LinearLayer(torch.nn.Linear):\n",
        "    \"\"\"Custom implementation of layer which includes the activation function\n",
        "    and its derivative\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        activation_function: torch.nn.Module,\n",
        "        last: bool = False,\n",
        "        order: int = 1,\n",
        "        bias: bool = True,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "\n",
        "        # Constructor for a regular linear layer\n",
        "        super().__init__(in_features, out_features, bias, device, dtype=torch.float64)\n",
        "\n",
        "        # Our modifications to the linear layer\n",
        "\n",
        "        # Store the activation function\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "        # Store the activation function's first derivative\n",
        "        self.activation_derivative = self.__get_activation_derivative()\n",
        "\n",
        "        # Store the activation function's second derivative\n",
        "        self.activation_2nd_derivative = self.__get_activation_2nd_derivative()\n",
        "\n",
        "        # Store the most recently computed\n",
        "        self.last = last\n",
        "        if order == 1:\n",
        "            self.forward = self.forward_dx\n",
        "        else:\n",
        "            self.forward = self.forward_ddx\n",
        "\n",
        "    def forward_dx(self, input: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        print(input)\n",
        "        x, dx = input[: input.shape[0] // 2], input[input.shape[0] // 2 :]\n",
        "        dim = self.weight.shape[0]\n",
        "\n",
        "        if self.last:\n",
        "            x = F.linear(x, self.weight, self.bias)\n",
        "            dx = F.linear(dx, self.weight, torch.zeros_like(self.bias))\n",
        "        else:\n",
        "            x = F.linear(x, self.weight, self.bias)\n",
        "            dx = self.activation_derivative(x) * F.linear(\n",
        "                dx, self.weight, torch.zeros_like(self.bias)\n",
        "            )\n",
        "            x = self.activation_function(x)\n",
        "\n",
        "        return torch.cat((x, dx), dim=0)\n",
        "\n",
        "    def forward_ddx(self, input: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        slicer = input.shape[0] // 3\n",
        "        x, dx, ddx = input[:slicer], input[slicer : 2 * slicer], input[2 * slicer :]\n",
        "\n",
        "        if self.last:\n",
        "            x = F.linear(x, self.weight, self.bias)\n",
        "            dx = F.linear(dx, self.weight, torch.zeros_like(self.bias))\n",
        "            ddx = F.linear(ddx, self.weight, torch.zeros_like(self.bias))\n",
        "        else:\n",
        "            x = F.linear(x, self.weight, self.bias)\n",
        "            dx_ = F.linear(dx, self.weight, torch.zeros_like(self.bias))\n",
        "            ddx_ = F.linear(ddx, self.weight, torch.zeros_like(self.bias))\n",
        "\n",
        "            dactivation = self.activation_derivative(x)\n",
        "            ddactivation = self.activation_2nd_derivative(x, dactivation)\n",
        "\n",
        "            dx = dactivation * dx_\n",
        "            ddx = ddactivation * dx_ + dactivation * ddx_\n",
        "\n",
        "            x = self.activation_function(x)\n",
        "\n",
        "        return torch.cat((x, dx, ddx), dim=0)\n",
        "\n",
        "    def __get_activation_derivative(self):\n",
        "        if isinstance(self.activation_function, torch.nn.ReLU):\n",
        "            return lambda x: torch.where(x > 0, torch.ones_like(x), torch.zeros_like(x))\n",
        "        if isinstance(self.activation_function, torch.nn.ELU):\n",
        "            return lambda x: torch.minimum(x, torch.exp(x))\n",
        "        if isinstance(self.activation_function, torch.nn.Sigmoid):\n",
        "            return lambda x: self.dsigmoid(x)\n",
        "\n",
        "    def __get_activation_2nd_derivative(self):\n",
        "        if isinstance(self.activation_function, torch.nn.ReLU):\n",
        "            return lambda x, dx=0: 0\n",
        "        if isinstance(self.activation_function, torch.nn.ELU):\n",
        "            return lambda x, dx=0: torch.where(x > 0, torch.exp(x), torch.zeros_like(x))\n",
        "        if isinstance(self.activation_function, torch.nn.Sigmoid):\n",
        "            return lambda x, dx=0: self.dsigmoid(dx)\n",
        "\n",
        "    def dsigmoid(self, x):\n",
        "        sigmoid = self.activation_function(x)\n",
        "        return sigmoid * (1 - sigmoid)\n",
        "        self.activation_2nd_derivative = self.__get_activation_2nd_derivative()\n",
        "\n",
        "        # Store the most recently computed\n",
        "        self.last = last\n",
        "        if order == 1:\n",
        "            self.forward = self.forward_dx\n",
        "        else:\n",
        "            self.forward = self.forward_ddx\n",
        "\n",
        "    def forward_dx(self, input: torch.Tensor):\n",
        "\n",
        "        print(input)\n",
        "        x, dx = input[: input.shape[0] // 2], input[input.shape[0] // 2 :]\n",
        "        dim = self.weight.shape[0]\n",
        "\n",
        "        if self.last:\n",
        "            x = F.linear(x, self.weight, self.bias)\n",
        "            dx = F.linear(dx, self.weight, torch.zeros_like(self.bias))\n",
        "        else:\n",
        "            x = F.linear(x, self.weight, self.bias)\n",
        "            dx = self.activation_derivative(x) * F.linear(\n",
        "                dx, self.weight, torch.zeros_like(self.bias)\n",
        "            )\n",
        "            x = self.activation_function(x)\n",
        "\n",
        "        return torch.cat((x, dx), dim=0)\n",
        "\n",
        "    def forward_ddx(self, input: torch.Tensor):\n",
        "\n",
        "        slicer = input.shape[0] // 3\n",
        "        x, dx, ddx = input[:slicer], input[slicer : 2 * slicer], input[2 * slicer :]\n",
        "\n",
        "        if self.last:\n",
        "            x = F.linear(x, self.weight, self.bias)\n",
        "            dx = F.linear(dx, self.weight, torch.zeros_like(self.bias))\n",
        "            ddx = F.linear(ddx, self.weight, torch.zeros_like(self.bias))\n",
        "        else:\n",
        "            x = F.linear(x, self.weight, self.bias)\n",
        "            dx_ = F.linear(dx, self.weight, torch.zeros_like(self.bias))\n",
        "            ddx_ = F.linear(ddx, self.weight, torch.zeros_like(self.bias))\n",
        "\n",
        "            dactivation = self.activation_derivative(x)\n",
        "            ddactivation = self.activation_2nd_derivative(x, dactivation)\n",
        "\n",
        "            dx = dactivation * dx_\n",
        "            ddx = ddactivation * dx_ + dactivation * ddx_\n",
        "\n",
        "            x = self.activation_function(x)\n",
        "\n",
        "        return torch.cat((x, dx, ddx), dim=0)\n",
        "\n",
        "    def __get_activation_derivative(self):\n",
        "        if isinstance(self.activation_function, torch.nn.ReLU):\n",
        "            return lambda x: torch.where(x > 0, torch.ones_like(x), torch.zeros_like(x))\n",
        "        if isinstance(self.activation_function, torch.nn.ELU):\n",
        "            return lambda x: torch.minimum(x, torch.exp(x))\n",
        "        if isinstance(self.activation_function, torch.nn.Sigmoid):\n",
        "            return lambda x: self.dsigmoid(x)\n",
        "\n",
        "    def __get_activation_2nd_derivative(self):\n",
        "        if isinstance(self.activation_function, torch.nn.ReLU):\n",
        "            return lambda x, dx=0: 0\n",
        "        if isinstance(self.activation_function, torch.nn.ELU):\n",
        "            return lambda x, dx=0: torch.where(x > 0, torch.exp(x), torch.zeros_like(x))\n",
        "        if isinstance(self.activation_function, torch.nn.Sigmoid):\n",
        "            return lambda x, dx=0: self.dsigmoid(dx)\n",
        "\n",
        "    def dsigmoid(self, x):\n",
        "        sigmoid = self.activation_function(x)\n",
        "        return sigmoid * (1 - sigmoid)"
      ],
      "metadata": {
        "id": "0Mu5l4NY_npI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function"
      ],
      "metadata": {
        "id": "_d327kpBAaaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        lambda_1: float,\n",
        "        lambda_2: float,\n",
        "        lambda_3: float,\n",
        "        lambda_r: float,\n",
        "        order: int = 1,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"Custom loss fucnction based on multiple MSEs\n",
        "\n",
        "        Args:\n",
        "            lambda_1 (float): loss weight decoder\n",
        "            lambda_2 (float): loss weight sindy z\n",
        "            lambda_3 (float): loss weight sindy x\n",
        "            lambda_r (float): loss weight sindy regularization\n",
        "            order (int, optional): Order of the model can be 1 or 2. Defaults to 1.\n",
        "        \"\"\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.lambda_1 = lambda_1\n",
        "        self.lambda_2 = lambda_2\n",
        "        self.lambda_3 = lambda_3\n",
        "        self.lambda_r = lambda_r\n",
        "\n",
        "        self.regularization = True\n",
        "\n",
        "        if order == 1:\n",
        "            self.forward = self.forward_dx\n",
        "        else:\n",
        "            self.forward = self.forward_ddx\n",
        "\n",
        "    def forward_dx(\n",
        "        self,\n",
        "        x,\n",
        "        dx,\n",
        "        dz,\n",
        "        dz_pred,\n",
        "        x_decode,\n",
        "        dx_decode,\n",
        "        sindy_coeffs: torch.Tensor,\n",
        "        coeff_mask,\n",
        "    ) -> torch.Tensor:\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        loss += self.lambda_1 * torch.mean((x - x_decode) ** 2)\n",
        "        loss += self.lambda_2 * torch.mean((dz - dz_pred) ** 2)\n",
        "        loss += self.lambda_3 * torch.mean((dx - dx_decode) ** 2)\n",
        "        loss += (\n",
        "            int(self.regularization)\n",
        "            * self.lambda_r\n",
        "            * torch.mean(torch.abs(sindy_coeffs) * coeff_mask)\n",
        "        )\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward_ddx(\n",
        "        self,\n",
        "        x,\n",
        "        dx,\n",
        "        dz,\n",
        "        dz_pred,\n",
        "        x_decode,\n",
        "        dx_decode,\n",
        "        sindy_coeffs: torch.Tensor,\n",
        "        ddz,\n",
        "        ddx,\n",
        "        ddx_decode,\n",
        "        coeff_mask,\n",
        "    ) -> torch.Tensor:\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        loss += self.lambda_1 * torch.mean((x - x_decode) ** 2)\n",
        "        # dz_pred is in this case ddz_pred\n",
        "        loss += self.lambda_2 * torch.mean((ddz - dz_pred) ** 2)\n",
        "        loss += self.lambda_3 * torch.mean((ddx - ddx_decode) ** 2)\n",
        "        loss += (\n",
        "            int(self.regularization)\n",
        "            * self.lambda_r\n",
        "            * torch.mean(torch.abs(sindy_coeffs) * coeff_mask)\n",
        "        )\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def set_regularization(self, include_regularization: bool) -> None:\n",
        "\n",
        "        self.regularization = include_regularization"
      ],
      "metadata": {
        "id": "kBHEldfwAcah"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder"
      ],
      "metadata": {
        "id": "T14FeSspA181"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(torch.nn.Module):\n",
        "\n",
        "    RELU = \"relu\"\n",
        "    SIGMOID = \"sigmoid\"\n",
        "    ELU = \"elu\"\n",
        "\n",
        "    def __init__(\n",
        "        self, params: dict = {}, name: str = \"encoder\", *args, **kwargs\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.params = params\n",
        "\n",
        "        activation = self.params[\"activation\"]\n",
        "        self.activation_function = self.__get_activation(activation)\n",
        "\n",
        "        self.weights = (\n",
        "            [self.params[\"input_dim\"]]\n",
        "            + self.params[\"widths\"]\n",
        "            + [self.params[\"latent_dim\"]]\n",
        "        )\n",
        "        self.order = self.params[\"model_order\"]\n",
        "\n",
        "        if self.weights is None:\n",
        "            raise TypeError(\"Missing weight param\")\n",
        "\n",
        "        if name == \"encoder\":\n",
        "            self.__create_encoder()\n",
        "        elif name == \"decoder\":\n",
        "            self.__create_decoder()\n",
        "\n",
        "    def __create_encoder(self) -> None:\n",
        "        \"\"\"Creates the encoder based on weights and activation function\"\"\"\n",
        "        layers = []\n",
        "        for curr_weights, next_weights in zip(self.weights[:-1], self.weights[1:]):\n",
        "            layers.append(\n",
        "                LinearLayer(\n",
        "                    curr_weights,\n",
        "                    next_weights,\n",
        "                    self.activation_function,\n",
        "                    len(layers) + 2 == len(self.weights),\n",
        "                    self.order,\n",
        "                )\n",
        "            )\n",
        "        self.net = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def __create_decoder(self) -> None:\n",
        "        \"\"\"Creates decoder, the weights are swapped and reversed compared to the encoder\"\"\"\n",
        "        layers = []\n",
        "        for curr_weights, next_weights in zip(\n",
        "            reversed(self.weights[1:]), reversed(self.weights[:-1])\n",
        "        ):\n",
        "            layers.append(\n",
        "                LinearLayer(\n",
        "                    curr_weights,\n",
        "                    next_weights,\n",
        "                    self.activation_function,\n",
        "                    len(layers) + 2 == len(self.weights),\n",
        "                    self.order,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.net = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def __get_activation(self, activation: str = \"relu\") -> torch.nn.Module:\n",
        "        match (activation):\n",
        "            case self.RELU:\n",
        "                return torch.nn.ReLU()\n",
        "            case self.SIGMOID:\n",
        "                return torch.nn.Sigmoid()\n",
        "            case self.ELU:\n",
        "                return torch.nn.ELU()\n",
        "            case _:\n",
        "                raise TypeError(f\"Invalid activation function {activation}\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> list[torch.Tensor]:\n",
        "        \"\"\"Forward function of the autoencoder\n",
        "\n",
        "        Args:\n",
        "            x (List[Tensor]): either the List has 2 or 3 elements\n",
        "            if it has 2 elements the model order has to be set to 1\n",
        "            if it has 3 elements the model order has to be set to 2\n",
        "\n",
        "        Returns:\n",
        "            List[torch.Tensor]: returns the forward passed list whit the same number of elements as the input\n",
        "        \"\"\"\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "uNUP6_KM7THr"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SINDy Network"
      ],
      "metadata": {
        "id": "YzcADr4vBJbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SINDy(torch.nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    Description: Custom neural network module that embeds a SINDy model into an autoencoder.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder: AutoEncoder,\n",
        "        decoder: AutoEncoder,\n",
        "        device: str,\n",
        "        params: dict = {},\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "\n",
        "        Description: Constructor for the SINDy class. Initializes the model parameters, encoder, and decoder.\n",
        "\n",
        "        Args:\n",
        "            encoder (AutoEncoder): The encoder part of the autoencoder.\n",
        "            decoder (AutoEncoder): The decoder part of the autoencoder.\n",
        "            params (Dict): A dictionary containing model and SINDy parameters.\n",
        "            *args: Additional positional arguments.\n",
        "            **kwargs: Additional keyword arguments.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Initialize model parameters, encoder, and decoder\n",
        "        self.params = params\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        # Initialize autoencoder parameters ----------\n",
        "        self.input_dim = self.params[\"input_dim\"]\n",
        "        self.latent_dim = self.params[\"latent_dim\"]\n",
        "\n",
        "        # Initialize SINDy parameters ------------------------------------------------------\n",
        "\n",
        "        # Library parameters\n",
        "        self.poly_order = self.params[\"poly_order\"]\n",
        "        if self.params[\"include_sine\"]:\n",
        "            self.include_sine = self.params[\"include_sine\"]\n",
        "        else:\n",
        "            self.include_sine = False\n",
        "        self.library_dim = library_size(\n",
        "            self.params[\"latent_dim\"], self.params[\"poly_order\"]\n",
        "        )\n",
        "\n",
        "        # Coefficient parameters\n",
        "        self.sequential_thresholding = self.params[\"sequential_thresholding\"]\n",
        "        self.coefficient_initialization = self.params[\"coefficient_initialization\"]\n",
        "        self.coefficient_mask = torch.ones((self.library_dim, self.latent_dim)).to(\n",
        "            device\n",
        "        )\n",
        "        self.coefficient_threshold = self.params[\"coefficient_threshold\"]\n",
        "\n",
        "        # Greek letter 'Xi' in the paper. Learned during training (different from linear regression).\n",
        "        sindy_coefficients = self.init_sindy_coefficients(\n",
        "            self.params[\"coefficient_initialization\"]\n",
        "        )\n",
        "        # Treat sindy_coefficients as a parameter to be learned and move it to device\n",
        "        self.sindy_coefficients = torch.nn.Parameter(\n",
        "            sindy_coefficients.to(torch.float64).to(device)\n",
        "        )\n",
        "\n",
        "        # Order of dynamical system\n",
        "        self.model_order = self.params[\"model_order\"]\n",
        "        if self.model_order == 1:\n",
        "            self.forward = self.forward_dx\n",
        "        else:\n",
        "            self.forward = self.forward_ddx\n",
        "\n",
        "    def init_sindy_coefficients(self, name=\"normal\", std=1.0, k=1) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "\n",
        "        Description: Initializes the SINDy coefficients based on the specified method. These coefficients are learned during training.\n",
        "\n",
        "        Args:\n",
        "            name (str): The method for initializing the coefficients. Options are 'xavier', 'uniform', 'constant', and 'normal'.\n",
        "            std (float): Standard deviation for normal initialization.\n",
        "            k (float): Constant value for constant initialization.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        sindy_coefficients = torch.zeros((self.library_dim, self.latent_dim))\n",
        "\n",
        "        if name == \"xavier\":\n",
        "            return torch.nn.init.xavier_uniform_(sindy_coefficients)\n",
        "        elif name == \"uniform\":\n",
        "            return torch.nn.init.uniform_(sindy_coefficients, low=0.0, high=1.0)\n",
        "        elif name == \"constant\":\n",
        "            return torch.ones_like(sindy_coefficients) * k\n",
        "        elif name == \"normal\":\n",
        "            return torch.nn.init.normal_(sindy_coefficients, mean=0, std=std)\n",
        "\n",
        "    def forward_dx(self, x, dx) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "\n",
        "        Description: Forward pass for the SINDy model with first-order derivatives.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor representing the state of the system.\n",
        "            dx (torch.Tensor): Input tensor representing the first-order derivatives of the state.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensors including the original state, first-order derivatives, predicted derivatives, and decoded states.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # pass input through encoder\n",
        "        out_encode = self.encoder(torch.cat((x, dx)))\n",
        "        dz = out_encode[out_encode.shape[0] // 2 :]\n",
        "        z = out_encode[: out_encode.shape[0] // 2]\n",
        "\n",
        "        # create library\n",
        "        Theta = sindy_library_pt(z, self.latent_dim, self.poly_order, self.include_sine)\n",
        "\n",
        "        # apply thresholding or not\n",
        "        if self.sequential_thresholding:\n",
        "            sindy_predict = torch.matmul(\n",
        "                Theta, self.coefficient_mask * self.sindy_coefficients\n",
        "            )\n",
        "        else:\n",
        "            sindy_predict = torch.matmul(Theta, self.sindy_coefficients)\n",
        "\n",
        "        # decode transformed input (z) and predicted derivatives (z dot)\n",
        "        x_decode = self.decoder(torch.cat((z, sindy_predict)))\n",
        "        dx_decode = x_decode[x_decode.shape[0] // 2 :]\n",
        "        x_decode = x_decode[: x_decode.shape[0] // 2]\n",
        "\n",
        "        dz_predict = sindy_predict\n",
        "\n",
        "        return (\n",
        "            x,\n",
        "            dx,\n",
        "            dz_predict,\n",
        "            dz,\n",
        "            x_decode,\n",
        "            dx_decode,\n",
        "            self.sindy_coefficients,\n",
        "        )\n",
        "\n",
        "    def forward_ddx(self, x: torch.Tensor, dx: torch.Tensor, ddx: torch.Tensor):\n",
        "        \"\"\"\n",
        "\n",
        "        Description: Forward pass for the SINDy model with second-order derivatives.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor representing the state of the system.\n",
        "            dx (torch.Tensor): Input tensor representing the first-order derivatives of the state.\n",
        "            ddx (torch.Tensor): Input tensor representing the second-order derivatives of the state.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        out = self.encoder(torch.cat((x, dx, ddx)))\n",
        "        slicer = out.shape[0] // 3\n",
        "        z, dz, ddz = out[:slicer], out[slicer : 2 * slicer], out[2 * slicer :]\n",
        "\n",
        "        # create Theta\n",
        "        Theta = sindy_library_pt_order2(\n",
        "            z, dz, self.latent_dim, self.poly_order, self.include_sine\n",
        "        )\n",
        "\n",
        "        # apply thresholding or not\n",
        "        if self.sequential_thresholding:\n",
        "            \"\"\"\n",
        "            tmp = torch.rand(size=(library_dim,latent_dim), dtype=torch.float32)\n",
        "            mask = torch.zeros_like(tmp)\n",
        "            mask = mask.where(self.coefficient_mask, tmp)\n",
        "            \"\"\"\n",
        "            mask = torch.where(\n",
        "                self.sindy_coefficients > self.coefficient_threshold,\n",
        "                self.sindy_coefficients,\n",
        "                0,\n",
        "            )\n",
        "            sindy_predict = torch.matmul(Theta, mask * self.sindy_coefficients)\n",
        "        else:\n",
        "            sindy_predict = torch.matmul(Theta, self.sindy_coefficients)\n",
        "\n",
        "        # decode\n",
        "        out_decode = self.decoder(torch.cat((z, dz, sindy_predict)))\n",
        "        slicer = out_decode.shape[0] // 3\n",
        "        x_decode, dx_decode, ddx_decode = (\n",
        "            out_decode[:slicer],\n",
        "            out_decode[slicer : 2 * slicer],\n",
        "            out_decode[2 * slicer :],\n",
        "        )\n",
        "\n",
        "        dz_predict = sindy_predict\n",
        "\n",
        "        return (\n",
        "            x,\n",
        "            dx,\n",
        "            dz_predict,\n",
        "            dz,\n",
        "            x_decode,\n",
        "            dx_decode,\n",
        "            self.sindy_coefficients,\n",
        "            sindy_predict,\n",
        "            z,\n",
        "        )"
      ],
      "metadata": {
        "id": "qfLlgt4vBVfg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "T6J0agtRCYKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_data['x'].shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ltpcVi0Jz5J",
        "outputId": "767767f8-7e47-4a26-97db-877540262411"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(sindy, num_epochs, optimizer, criterion):\n",
        "    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
        "        sindy.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        (\n",
        "            x,\n",
        "            dx,\n",
        "            dz_predict,\n",
        "            dz,\n",
        "            x_decode,\n",
        "            dx_decode,\n",
        "            sindy_coefficients,\n",
        "        ) = sindy(\n",
        "            torch.from_numpy(training_data[\"x\"]).to(device=device),\n",
        "            torch.from_numpy(training_data[\"dx\"]).to(device=device),\n",
        "        )\n",
        "        loss = criterion(\n",
        "            x,\n",
        "            dx,\n",
        "            dz,\n",
        "            dz_predict,\n",
        "            x_decode,\n",
        "            dx_decode,\n",
        "            sindy_coefficients,\n",
        "            sindy.coefficient_mask,\n",
        "        )\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Optional coefficient thresholding\n",
        "        if (\n",
        "            criterion.regularization\n",
        "            and params[\"sequential_thresholding\"]\n",
        "            and (epoch % params[\"threshold_frequency\"] == 0)\n",
        "            and (epoch > 0)\n",
        "        ):\n",
        "            sindy.coefficient_mask = (\n",
        "                torch.abs(sindy_coefficients) > params[\"coefficient_threshold\"]\n",
        "            )\n",
        "            print(\n",
        "                f\"THRESHOLDING: {torch.sum(sindy.coefficient_mask)} active coefficients\"\n",
        "            )\n",
        "            dill.dump(sindy, open(f\"model_lorenz_1_{epoch}\", \"wb\"))\n"
      ],
      "metadata": {
        "id": "xjbu69LICZ8n"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = json.load(open(\"params.json\"))\n",
        "encoder = AutoEncoder(params, \"encoder\")\n",
        "decoder = AutoEncoder(params, \"decoder\")\n",
        "sindy = SINDy(encoder=encoder, decoder=decoder, device=device, params=params)\n",
        "sindy = sindy.to(device=device)\n",
        "criterion = Loss(\n",
        "    params[\"loss_weight_decoder\"],\n",
        "    params[\"loss_weight_sindy_z\"],\n",
        "    params[\"loss_weight_sindy_x\"],\n",
        "    params[\"loss_weight_sindy_regularization\"],\n",
        ")\n",
        "optimizer = torch.optim.Adam(sindy.parameters(), lr=params[\"learning_rate\"])\n",
        "num_epochs = training_data['x'].shape[0]\n",
        "\n",
        "\n",
        "### Train with regularization\n",
        "criterion.set_regularization(True)\n",
        "train(sindy, num_epochs, optimizer, criterion)\n",
        "\n",
        "dill.dump(sindy, open(f\"model_lorenz_1\", \"wb\"))"
      ],
      "metadata": {
        "id": "RjTSqmpkDfSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}